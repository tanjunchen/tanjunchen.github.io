<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="漫步远方，心荡神往"><meta property="og:type" content="article"><meta property="og:image" content="https://tanjunchen.github.io/img/home.webp"><meta property="twitter:image" content="https://tanjunchen.github.io/img/home.webp"><meta name=title content="LLM 教程（1）- DeepSeek-R1 初步入门"><meta property="og:title" content="LLM 教程（1）- DeepSeek-R1 初步入门"><meta property="twitter:title" content="LLM 教程（1）- DeepSeek-R1 初步入门"><meta name=description content="陈谭军，软件工程师, 开源爱好者, 互联网, 云原生, 容器, 微服务, Web, PaaS, Istio, Kubernetes, Microservice"><meta property="og:description" content="陈谭军，软件工程师, 开源爱好者, 互联网, 云原生, 容器, 微服务, Web, PaaS, Istio, Kubernetes, Microservice"><meta property="twitter:description" content="陈谭军，软件工程师, 开源爱好者, 互联网, 云原生, 容器, 微服务, Web, PaaS, Istio, Kubernetes, Microservice"><meta property="twitter:card" content="summary"><meta name=keyword content="陈谭军, 互联网, 云原生, 容器, 微服务, Web, PaaS, Istio, Kubernetes, Microservice"><link rel="shortcut icon" href=/img/favicon.ico><title>LLM 教程（1）- DeepSeek-R1 初步入门 | 陈谭军的博客 | tanjunchen Blog</title><link rel=canonical href=/post/2025-03-01-llm-learn1/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script>
<script src=/js/bootstrap.min.js></script>
<script src=/js/hux-blog.min.js></script>
<script src=/js/lazysizes.min.js></script></head><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button>
<a class=navbar-brand href=/>漫步远方，心荡神往</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/technology/>technology</a></li><li><a href=/categories/think/>think</a></li><li><a href=/learning/>LEARNING</a></li><li><a href=/archive/>ARCHIVE</a></li><li><a href=/about/>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(/img/home.webp)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/ai title=AI>AI</a>
<a class=tag href=/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B title=大模型>大模型</a>
<a class=tag href=/tags/deepseek title=DeepSeek>DeepSeek</a></div><h1>LLM 教程（1）- DeepSeek-R1 初步入门</h1><h2 class=subheading>DeepSeek-R1 基础知识，开源权重、模型系列、入门介绍等</h2><span class=meta>Posted by
陈谭军
on
Saturday, March 1, 2025
<span id=busuanzi_container_page_pv>|<span id=busuanzi_value_page_pv></span><span>
<span id=/post/2025-03-01-llm-learn1/ class="leancloud_visitors meta_data_item" data-flag-title><span class=post-meta-item-icon><span class="octicon octicon-eye"></span></span>
<i class="fa fa-eye"></i>
<span class=old-visitors-count style=display:none></span>
<span class=leancloud-visitors-count></span></span>
<script src=https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js></script>
<script>AV.initialize("","")</script><script type=text/javascript>function showTime(e){var n=new AV.Query(e),t=[],s=$(".leancloud_visitors");s.each(function(){t.push($(this).attr("id").trim())}),n.containedIn("url",t),n.find().done(function(e){for(var s,o,i,a,r,c,l=".leancloud-visitors-count",d=".old-visitors-count",n=0;n<e.length;n++)a=e[n],i=a.get("url"),c=a.get("time"),s=document.getElementById(i),$(s).find(l).text(c);for(n=0;n<t.length;n++)i=t[n],s=document.getElementById(i),o=$(s).find(l),o.text()==""&&(r=$(s).find(d).text(),r!=""?o.text(0+parseInt(r)):o.text(0))}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var n=$(".leancloud_visitors"),t=n.attr("id").trim(),o=n.attr("data-flag-title").trim(),s=new AV.Query(e);s.equalTo("url",t),s.find({success:function(n){if(n.length>0){var s,i,r,c,l,a=n[0];a.fetchWhenSave(!0),a.increment("time"),a.save(null,{success:function(e){var n=$(document.getElementById(t));n.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else s=new e,i=new AV.ACL,i.setPublicReadAccess(!0),i.setPublicWriteAccess(!0),s.setACL(i),s.set("title",o),s.set("url",t),c=".old-visitors-count",l=$(document.getElementById(t)),r=l.find(c).text(),r!=""?s.set("time",parseInt(r)+1):s.set("time",1),s.save(null,{success:function(e){var n=$(document.getElementById(t));n.find(".leancloud-visitors-count").text(e.get("time"))},error:function(){console.log("Failed to create")}})},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");$(".leancloud_visitors").length==1?addCount(e):showTime(e)})</script>阅读 </span></span>|<span class=post-date>共 4932 字</span>，阅读约 <span class=more-meta>10 分钟</span></span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><h1 id=基础知识>基础知识</h1><p><img src=/images/2025-03-01-llm-learn1/1.png alt></p><p>查看 <a href=https://huggingface.co/deepseek-ai>deepseek-ai 开源官网</a>，DeepSeek 有以下系列：</p><ul><li>DeepSeek-R1</li><li>DeepSeek-V3 （DeepSeek-V3-Base）</li><li>DeepSeek-VL</li><li>DeepSeek-Coder</li><li>DeepSeek-Math</li><li>DeepSeek-LLM</li><li>蒸馏模型系列（Qwen、LLaMA等）</li><li>&mldr;&mldr;</li></ul><p><img src=/images/2025-03-01-llm-learn1/2.png alt></p><h1 id=模型系列>模型系列</h1><table><thead><tr><th>模型名称</th><th>类型/架构</th><th>参数规模</th><th>核心能力</th><th>关键技术</th></tr></thead><tbody><tr><td>DeepSeek-R1</td><td>混合专家（MoE）</td><td>总参数671B<br>激活参数37B/token<br>支持128K上下文</td><td>• 对标GPT-4o/Claude 3.5 Sonnet<br>• 数学/编程任务领先<br>• 完整开源支持</td><td>• 动态负载均衡<br>• 多词元预测(MTP)<br>• FP8混合精度训练（成本$557万）</td></tr><tr><td>DeepSeek-V3</td><td>纯强化学习（RL）</td><td>671B基础版<br>1.5B-70B蒸馏版</td><td>• 无需SFT直接RL优化<br>• 数学/编程达OpenAI o1水平</td><td>• 群组相对策略优化(GRPO)<br>• 双奖励机制（准确度+格式）</td></tr><tr><td>Janus-Pro-7B</td><td>多模态统一架构</td><td>7B参数<br>单卡部署（≥24GB）</td><td>• 视觉问答超GPT-4V<br>• 文生图质量优于DALL·E 3<br>• 逆向图像生成代码</td><td>• 自回归多模态框架<br>• 企业级隐私保障<br>• 本地部署方案</td></tr><tr><td>DeepSeek-VL2</td><td>MoE视觉架构</td><td>未披露</td><td>• 梗图解析<br>• 多图生成故事</td><td>• 视觉编码解耦设计<br>• 多模态任务冲突缓解</td></tr><tr><td>DeepSeek-V2.5</td><td>融合对话与代码</td><td>未披露</td><td>• 通用对话+代码生成<br>• 防御越狱攻击</td><td>• 支持Function Calling<br>• 多语言API接口<br>• FIM补全机制</td></tr><tr><td>蒸馏模型系列</td><td>轻量级推理模型</td><td>1.5B/7B/14B/32B/70B</td><td>• 继承R1推理能力<br>• 部分版本超GPT-4o<br>• 开源商用</td><td>• 知识蒸馏技术<br>• 边缘设备适配</td></tr></tbody></table><p>DeepSeek的模型集合覆盖语言、推理、多模态三大领域，核心优势在于高性能、低成本、全开源，其技术突破包括：</p><ul><li>训练效率 FP8混合精度 + DualPipe流水线并行 + 千卡集群线性加速效率92%</li><li>推理优化 RL直接策略优化 + 多词元预测加速解码</li><li>多模态融合 自回归框架统一模态交互 + 视觉-语言耦合参数共享</li></ul><h1 id=deepseek-r1>DeepSeek-R1</h1><h2 id=入门介绍>入门介绍</h2><p>DeepSeek 推出了推理模型 DeepSeek-R1-Zero 和 DeepSeek-R1：</p><ul><li>DeepSeek-R1-Zero：该模型通过大规模强化学习（RL）训练而成，无需监督微调（SFT）作为前置步骤，在推理任务中展现了卓越的性能。通过RL训练，DeepSeek-R1-Zero 自然涌现出许多强大且有趣的推理行为。然而，它也面临一些挑战，例如无限重复、可读性差、语言混杂等问题。</li><li>DeepSeek-R1：为了解决这些问题并进一步提升推理性能，在RL训练前引入了冷启动数据。DeepSeek-R1 在数学、代码和通用推理任务中达到了与 OpenAI-o1 相当的水平。</li></ul><p>开源贡献：</p><ul><li>开源了 DeepSeek-R1-Zero 和 DeepSeek-R1 的完整模型权重。</li><li>基于 Llama 和 Qwen 架构，还提供了6个从 DeepSeek-R1 蒸馏的轻量级模型。</li><li>性能突破：其中，DeepSeek-R1-Distill-Qwen-32B 在多项基准测试中超越了 OpenAI-o1-mini，为稠密模型（Dense Model）树立了新的性能标杆。</li></ul><p><img src=/images/2025-03-01-llm-learn1/3.png alt></p><h2 id=精读简要>精读简要</h2><ul><li>Post-Training：基于大规模强化学习的模型优化；</li></ul><p>DeepSeek 直接在基础模型上应用强化学习（RL），而无需依赖监督微调（SFT）作为前置步骤。这种方法使得模型能够探索思维链（CoT）来解决复杂问题，从而开发出了 DeepSeek-R1-Zero。DeepSeek-R1-Zero 展现了自我验证、反思以及生成长思维链的能力，这标志着研究领域的一个重要里程碑。值得注意的是，这是首个公开的研究成果，验证了大语言模型（LLMs）的推理能力可以完全通过强化学习激励，而无需依赖监督微调（SFT）。这一突破为未来在该领域的进一步发展铺平了道路。</p><p>DeepSeek 还介绍了开发 DeepSeek-R1 的流程。该流程包含两个RL阶段，旨在发现更优的推理模式并与人类偏好对齐，以及两个SFT阶段，作为模型推理和非推理能力的种子，这一流程将通过创建更好的模型为行业带来益处。</p><ul><li>蒸馏：小模型也可以很强大</li></ul><p>DeepSeek 证明了大模型的推理模式可以通过蒸馏技术迁移到小模型中，从而使这些小模型的性能优于直接通过强化学习（RL）在小模型上发现的推理模式。开源的 DeepSeek-R1 及其API将为研究社区提供支持，帮助未来蒸馏出更优秀的小模型。</p><p>DeepSeek 利用 DeepSeek-R1 生成的推理数据，对研究社区广泛使用的多个稠密模型进行了微调。评估结果表明，经过蒸馏的小型稠密模型在基准测试中表现优异。DeepSeek 向社区开源了基于 Qwen2.5 和 Llama3 系列的蒸馏模型，参数规模包括 1.5B、7B、8B、14B、32B 和 70B 的检查点。</p><p>DeepSeek-R1-Zero和DeepSeek-R1是基于DeepSeek-V3-Base模型进行训练的。</p><p><img src=/images/2025-03-01-llm-learn1/4.png alt></p><p>DeepSeek-R1-Distill 模型是基于开源模型进行微调的，使用了DeepSeek-R1生成的样本。DeepSeek对这些模型的配置和分词器进行了轻微调整。</p><p><img src=/images/2025-03-01-llm-learn1/5.png alt></p><h2 id=商业协议>商业协议</h2><p>DeepSeek 及模型权重遵循 MIT 许可证。DeepSeek-R1 系列支持商业用途，允许任何形式的修改和衍生作品，包括但不限于用于蒸馏训练其他大语言模型（LLMs）。请注意以下事项：</p><ul><li>DeepSeek-R1-Distill-Qwen-1.5B、DeepSeek-R1-Distill-Qwen-7B、DeepSeek-R1-Distill-Qwen-14B 和 DeepSeek-R1-Distill-Qwen-32B 是基于 Qwen-2.5 系列开发的，其原始许可证为 Apache 2.0，现使用 DeepSeek-R1 生成的 80 万条样本进行了微调。</li><li>DeepSeek-R1-Distill-Llama-8B 是基于 Llama3.1-8B-Base 开发的，其原始许可证为 llama3.1 许可证。</li><li>DeepSeek-R1-Distill-Llama-70B 是基于 Llama3.3-70B-Instruct 开发的，其原始许可证为 llama3.3 许可证。</li></ul><h2 id=本地部署>本地部署</h2><p>DeepSeek-R1 与 DeepSeek-V3 可以通过以下硬件和开源社区软件进行本地部署：</p><ul><li>DeepSeek-Infer Demo：DeepSeek 提供了一个简单轻量的演示，支持 FP8 和 BF16 推理。</li><li>SGLang：全面支持 DeepSeek-V3 模型的 BF16 和 FP8 推理模式。</li><li>LMDeploy：支持高效的 FP8 和 BF16 推理，适用于本地和云端部署。</li><li>TensorRT-LLM：目前支持 BF16 推理和 INT4/8 量化，FP8 支持即将推出。</li><li>vLLM：支持 DeepSeek-V3 模型的 FP8 和 BF16 模式，适用于张量并行和流水线并行。</li><li>AMD GPU：通过 SGLang 支持在 AMD GPU 上以 BF16 和 FP8 模式运行 DeepSeek-V3 模型。</li><li>华为昇腾 NPU：支持在华为昇腾设备上运行 DeepSeek-V3。</li></ul><p>DeepSeek-R1-Distill 模型的使用方式与 Qwen 或 Llama 模型相同，如下所示：</p><p>vLLM：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size <span style=color:#bd93f9>2</span> --max-model-len <span style=color:#bd93f9>32768</span> --enforce-eager
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 或者如下启动命令</span>
</span></span><span style=display:flex><span>python3 -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port <span style=color:#bd93f9>12345</span> --max-model-len <span style=color:#bd93f9>65536</span> --trust-remote-code --tensor-parallel-size <span style=color:#bd93f9>8</span> --quantization moe_wna16 --gpu-memory-utilization 0.90 --kv-cache-dtype fp8_e5m2 --calculate-kv-scales --served-model-name deepseek-reasoner --model /workspace/DeepSeek-R1-awq
</span></span></code></pre></div><p>SGLang：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp <span style=color:#bd93f9>2</span>
</span></span></code></pre></div><h1 id=配置说明>配置说明</h1><p>扒一扒 DeepSeek 开源文件，如下所示：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>-rw-r--r-- <span style=color:#bd93f9>1</span> root root       <span style=color:#bd93f9>1729</span> Feb <span style=color:#bd93f9>12</span> 14:13 config.json
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#bd93f9>1</span> root root      <span style=color:#bd93f9>10556</span> Feb <span style=color:#bd93f9>12</span> 14:13 configuration_deepseek.py
</span></span><span style=display:flex><span>drwxr-xr-x <span style=color:#bd93f9>2</span> root root       <span style=color:#bd93f9>4096</span> Feb <span style=color:#bd93f9>12</span> 14:11 figures
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#bd93f9>1</span> root root        <span style=color:#bd93f9>171</span> Feb <span style=color:#bd93f9>12</span> 14:13 generation_config.json
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#bd93f9>1</span> root root       <span style=color:#bd93f9>1064</span> Feb <span style=color:#bd93f9>12</span> 14:12 LICENSE
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#bd93f9>1</span> root root <span style=color:#bd93f9>5234139343</span> Feb <span style=color:#bd93f9>12</span> 14:17 xxxxx.safetensors 
</span></span><span style=display:flex><span>.............................................................
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#bd93f9>1</span> root root      <span style=color:#bd93f9>75769</span> Feb <span style=color:#bd93f9>12</span> 14:19 modeling_deepseek.py
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#bd93f9>1</span> root root    <span style=color:#bd93f9>8898324</span> Feb <span style=color:#bd93f9>12</span> 14:08 model.safetensors.index.json
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#bd93f9>1</span> root root      <span style=color:#bd93f9>18582</span> Feb <span style=color:#bd93f9>12</span> 14:17 README.md
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#bd93f9>1</span> root root       <span style=color:#bd93f9>3584</span> Feb <span style=color:#bd93f9>12</span> 14:25 tokenizer_config.json
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#bd93f9>1</span> root root    <span style=color:#bd93f9>7847602</span> Feb <span style=color:#bd93f9>12</span> 14:13 tokenizer.json
</span></span></code></pre></div><ul><li>模型权重 (.safetensors)<ul><li>xxx.safetensors</li><li>作用：存储神经网络的参数（如权重和偏置），是 DeepSeek 训练好的模型核心数据。</li><li>格式：.safetensors 是一种高效、安全的张量存储格式，比 .bin 更快，避免了 pickle 相关的安全问题。</li></ul></li><li>配置文件<ul><li>config.json：定义模型架构（如层数、隐藏单元、注意力头数等）。</li><li>generation_config.json：控制文本生成参数（如最大长度、温度、top-k、top-p 等）。</li><li>tokenizer_config.json：定义分词器的相关参数。</li></ul></li><li>模型代码<ul><li>modeling_deepseek.py：实现模型的 前向传播（forward），定义了模型的结构。</li><li>configuration_deepseek.py：模型的初始化配置，加载参数用的。</li><li>tokenizer.json：存储分词器（Tokenizer），用于将文本转换为模型可理解的 token。</li></ul></li><li>附加文件<ul><li>README.md：文档说明。</li><li>LICENSE：开源许可证。</li><li>figures/：可能包含模型架构或训练过程的可视化图表。</li></ul></li></ul><p>config.json 的配置文件如下所示：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;architectures&#34;</span>: [
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;DeepseekV3ForCausalLM&#34;</span>
</span></span><span style=display:flex><span>  ],
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;attention_bias&#34;</span>: <span style=color:#ff79c6>false</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;attention_dropout&#34;</span>: <span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;auto_map&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;AutoConfig&#34;</span>: <span style=color:#f1fa8c>&#34;configuration_deepseek.DeepseekV3Config&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;AutoModel&#34;</span>: <span style=color:#f1fa8c>&#34;modeling_deepseek.DeepseekV3Model&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;AutoModelForCausalLM&#34;</span>: <span style=color:#f1fa8c>&#34;modeling_deepseek.DeepseekV3ForCausalLM&#34;</span>
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;aux_loss_alpha&#34;</span>: <span style=color:#bd93f9>0.001</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;bos_token_id&#34;</span>: <span style=color:#bd93f9>0</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;eos_token_id&#34;</span>: <span style=color:#bd93f9>1</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;ep_size&#34;</span>: <span style=color:#bd93f9>1</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;first_k_dense_replace&#34;</span>: <span style=color:#bd93f9>3</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;hidden_act&#34;</span>: <span style=color:#f1fa8c>&#34;silu&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;hidden_size&#34;</span>: <span style=color:#bd93f9>7168</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;initializer_range&#34;</span>: <span style=color:#bd93f9>0.02</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;intermediate_size&#34;</span>: <span style=color:#bd93f9>18432</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;kv_lora_rank&#34;</span>: <span style=color:#bd93f9>512</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;max_position_embeddings&#34;</span>: <span style=color:#bd93f9>163840</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;model_type&#34;</span>: <span style=color:#f1fa8c>&#34;deepseek_v3&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;moe_intermediate_size&#34;</span>: <span style=color:#bd93f9>2048</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;moe_layer_freq&#34;</span>: <span style=color:#bd93f9>1</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;n_group&#34;</span>: <span style=color:#bd93f9>8</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;n_routed_experts&#34;</span>: <span style=color:#bd93f9>256</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;n_shared_experts&#34;</span>: <span style=color:#bd93f9>1</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;norm_topk_prob&#34;</span>: <span style=color:#ff79c6>true</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;num_attention_heads&#34;</span>: <span style=color:#bd93f9>128</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;num_experts_per_tok&#34;</span>: <span style=color:#bd93f9>8</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;num_hidden_layers&#34;</span>: <span style=color:#bd93f9>61</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;num_key_value_heads&#34;</span>: <span style=color:#bd93f9>128</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;num_nextn_predict_layers&#34;</span>: <span style=color:#bd93f9>1</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;pretraining_tp&#34;</span>: <span style=color:#bd93f9>1</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;q_lora_rank&#34;</span>: <span style=color:#bd93f9>1536</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;qk_nope_head_dim&#34;</span>: <span style=color:#bd93f9>128</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;qk_rope_head_dim&#34;</span>: <span style=color:#bd93f9>64</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;quantization_config&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;activation_scheme&#34;</span>: <span style=color:#f1fa8c>&#34;dynamic&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;fmt&#34;</span>: <span style=color:#f1fa8c>&#34;e4m3&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;quant_method&#34;</span>: <span style=color:#f1fa8c>&#34;fp8&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;weight_block_size&#34;</span>: [
</span></span><span style=display:flex><span>      <span style=color:#bd93f9>128</span>,
</span></span><span style=display:flex><span>      <span style=color:#bd93f9>128</span>
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;rms_norm_eps&#34;</span>: <span style=color:#bd93f9>1e-06</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;rope_scaling&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;beta_fast&#34;</span>: <span style=color:#bd93f9>32</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;beta_slow&#34;</span>: <span style=color:#bd93f9>1</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;factor&#34;</span>: <span style=color:#bd93f9>40</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;mscale&#34;</span>: <span style=color:#bd93f9>1.0</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;mscale_all_dim&#34;</span>: <span style=color:#bd93f9>1.0</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;original_max_position_embeddings&#34;</span>: <span style=color:#bd93f9>4096</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;yarn&#34;</span>
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;rope_theta&#34;</span>: <span style=color:#bd93f9>10000</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;routed_scaling_factor&#34;</span>: <span style=color:#bd93f9>2.5</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;scoring_func&#34;</span>: <span style=color:#f1fa8c>&#34;sigmoid&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;seq_aux&#34;</span>: <span style=color:#ff79c6>true</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;tie_word_embeddings&#34;</span>: <span style=color:#ff79c6>false</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;topk_group&#34;</span>: <span style=color:#bd93f9>4</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;topk_method&#34;</span>: <span style=color:#f1fa8c>&#34;noaux_tc&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;torch_dtype&#34;</span>: <span style=color:#f1fa8c>&#34;bfloat16&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;transformers_version&#34;</span>: <span style=color:#f1fa8c>&#34;4.46.3&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;use_cache&#34;</span>: <span style=color:#ff79c6>true</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;v_head_dim&#34;</span>: <span style=color:#bd93f9>128</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;vocab_size&#34;</span>: <span style=color:#bd93f9>129280</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=模型架构>模型架构</h2><table><thead><tr><th>参数</th><th>值</th><th>技术说明</th></tr></thead><tbody><tr><td>model_type</td><td>deepseek_v3</td><td>模型系列标识，代表第三代深度搜索架构</td></tr><tr><td>architectures</td><td>[&ldquo;DeepseekV3ForCausalLM&rdquo;]</td><td>自回归文本生成专用架构</td></tr><tr><td>num_hidden_layers</td><td>61</td><td>深层Transformer结构（约2倍于Llama3-70B）</td></tr><tr><td>hidden_size</td><td>7168</td><td>隐藏层维度（Llama3两倍：Llama3-70B为8192）</td></tr><tr><td>intermediate_size</td><td>18432</td><td>FFN层扩展维度（计算公式：hidden_size * 2.57）</td></tr><tr><td>vocab_size</td><td>129280</td><td>支持多语言的大词表（含中文/代码/数学符号）</td></tr></tbody></table><h2 id=注意力机制>注意力机制</h2><table><thead><tr><th>参数</th><th>值</th><th>技术特性</th></tr></thead><tbody><tr><td>num_attention_heads</td><td>128</td><td>多头注意力机制（每头维度56）</td></tr><tr><td>num_key_value_heads</td><td>128</td><td>Key-Value头数（与注意力头1:1对应）</td></tr><tr><td>attention_dropout</td><td>0.0</td><td>关闭注意力随机丢弃（提升推理稳定性）</td></tr><tr><td>qk_nope_head_dim</td><td>128</td><td>标准注意力头维度</td></tr><tr><td>qk_rope_head_dim</td><td>64</td><td>带旋转位置编码的头维度</td></tr></tbody></table><h2 id=位置编码-rope>位置编码 (RoPE)</h2><table><thead><tr><th>参数</th><th>值</th><th>实现细节</th></tr></thead><tbody><tr><td>max_position_embeddings</td><td>163840</td><td>支持163K tokens长上下文（≈12万汉字）</td></tr><tr><td>rope_theta</td><td>10000</td><td>位置编码基频参数</td></tr><tr><td>rope_scaling.type</td><td>yarn</td><td>采用YAReN（Yet Another RoPE Extension）扩展方法</td></tr><tr><td>rope_scaling.factor</td><td>40</td><td>位置编码扩展系数（原始4096→扩展163840）</td></tr><tr><td>rope_scaling.beta_fast</td><td>32</td><td>高频分量衰减系数</td></tr><tr><td>rope_scaling.beta_slow</td><td>1</td><td>低频分量保持系数</td></tr></tbody></table><h2 id=专家混合系统-moe>专家混合系统 (MoE)</h2><table><thead><tr><th>参数</th><th>值</th><th>设计特点</th></tr></thead><tbody><tr><td>n_routed_experts</td><td>256</td><td>专家总数（每层）</td></tr><tr><td>num_experts_per_tok</td><td>8</td><td>每个token路由8个专家</td></tr><tr><td>topk_group</td><td>4</td><td>专家选择分组数</td></tr><tr><td>routed_scaling_factor</td><td>2.5</td><td>专家输出加权系数</td></tr></tbody></table><h2 id=量化与优化>量化与优化</h2><table><thead><tr><th>参数</th><th>值</th><th>实现方案</th></tr></thead><tbody><tr><td>torch_dtype</td><td>bfloat16</td><td>基础训练精度</td></tr><tr><td>quantization_config</td><td>FP8</td><td>支持动态8位浮点量化</td></tr><tr><td>q_lora_rank</td><td>1536</td><td>Query投影矩阵低秩适配</td></tr><tr><td>kv_lora_rank</td><td>512</td><td>Key-Value投影低秩优化</td></tr></tbody></table><p>tokenizer_config.json 配置文件如下所示：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;add_bos_token&#34;</span>: <span style=color:#ff79c6>true</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;add_eos_token&#34;</span>: <span style=color:#ff79c6>false</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;bos_token&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;__type&#34;</span>: <span style=color:#f1fa8c>&#34;AddedToken&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;content&#34;</span>: <span style=color:#f1fa8c>&#34;&lt;｜begin▁of▁sentence｜&gt;&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;lstrip&#34;</span>: <span style=color:#ff79c6>false</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;normalized&#34;</span>: <span style=color:#ff79c6>true</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;rstrip&#34;</span>: <span style=color:#ff79c6>false</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;single_word&#34;</span>: <span style=color:#ff79c6>false</span>
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;clean_up_tokenization_spaces&#34;</span>: <span style=color:#ff79c6>false</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;eos_token&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;__type&#34;</span>: <span style=color:#f1fa8c>&#34;AddedToken&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;content&#34;</span>: <span style=color:#f1fa8c>&#34;&lt;｜end▁of▁sentence｜&gt;&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;lstrip&#34;</span>: <span style=color:#ff79c6>false</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;normalized&#34;</span>: <span style=color:#ff79c6>true</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;rstrip&#34;</span>: <span style=color:#ff79c6>false</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;single_word&#34;</span>: <span style=color:#ff79c6>false</span>
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;legacy&#34;</span>: <span style=color:#ff79c6>true</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;model_max_length&#34;</span>: <span style=color:#bd93f9>16384</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;pad_token&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;__type&#34;</span>: <span style=color:#f1fa8c>&#34;AddedToken&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;content&#34;</span>: <span style=color:#f1fa8c>&#34;&lt;｜end▁of▁sentence｜&gt;&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;lstrip&#34;</span>: <span style=color:#ff79c6>false</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;normalized&#34;</span>: <span style=color:#ff79c6>true</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;rstrip&#34;</span>: <span style=color:#ff79c6>false</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;single_word&#34;</span>: <span style=color:#ff79c6>false</span>
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;sp_model_kwargs&#34;</span>: {},
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;unk_token&#34;</span>: <span style=color:#ff79c6>null</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;tokenizer_class&#34;</span>: <span style=color:#f1fa8c>&#34;LlamaTokenizerFast&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;chat_template&#34;</span>: <span style=color:#f1fa8c>&#34;{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt=&#39;&#39;, is_first_sp=true) %}{%- for message in messages %}{%- if message[&#39;role&#39;] == &#39;system&#39; %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[&#39;content&#39;] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + &#39;\\n\\n&#39; + message[&#39;content&#39;] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message[&#39;role&#39;] == &#39;user&#39; %}{%- set ns.is_tool = false -%}{{&#39;&lt;｜User｜&gt;&#39; + message[&#39;content&#39;]}}{%- endif %}{%- if message[&#39;role&#39;] == &#39;assistant&#39; and &#39;tool_calls&#39; in message %}{%- set ns.is_tool = false -%}{%- for tool in message[&#39;tool_calls&#39;] %}{%- if not ns.is_first %}{%- if message[&#39;content&#39;] is none %}{{&#39;&lt;｜Assistant｜&gt;&lt;｜tool▁calls▁begin｜&gt;&lt;｜tool▁call▁begin｜&gt;&#39; + tool[&#39;type&#39;] + &#39;&lt;｜tool▁sep｜&gt;&#39; + tool[&#39;function&#39;][&#39;name&#39;] + &#39;\\n&#39; + &#39;```json&#39; + &#39;\\n&#39; + tool[&#39;function&#39;][&#39;arguments&#39;] + &#39;\\n&#39; + &#39;```&#39; + &#39;&lt;｜tool▁call▁end｜&gt;&#39;}}{%- else %}{{&#39;&lt;｜Assistant｜&gt;&#39; + message[&#39;content&#39;] + &#39;&lt;｜tool▁calls▁begin｜&gt;&lt;｜tool▁call▁begin｜&gt;&#39; + tool[&#39;type&#39;] + &#39;&lt;｜tool▁sep｜&gt;&#39; + tool[&#39;function&#39;][&#39;name&#39;] + &#39;\\n&#39; + &#39;```json&#39; + &#39;\\n&#39; + tool[&#39;function&#39;][&#39;arguments&#39;] + &#39;\\n&#39; + &#39;```&#39; + &#39;&lt;｜tool▁call▁end｜&gt;&#39;}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{&#39;\\n&#39; + &#39;&lt;｜tool▁call▁begin｜&gt;&#39; + tool[&#39;type&#39;] + &#39;&lt;｜tool▁sep｜&gt;&#39; + tool[&#39;function&#39;][&#39;name&#39;] + &#39;\\n&#39; + &#39;```json&#39; + &#39;\\n&#39; + tool[&#39;function&#39;][&#39;arguments&#39;] + &#39;\\n&#39; + &#39;```&#39; + &#39;&lt;｜tool▁call▁end｜&gt;&#39;}}{%- endif %}{%- endfor %}{{&#39;&lt;｜tool▁calls▁end｜&gt;&lt;｜end▁of▁sentence｜&gt;&#39;}}{%- endif %}{%- if message[&#39;role&#39;] == &#39;assistant&#39; and &#39;tool_calls&#39; not in message %}{%- if ns.is_tool %}{{&#39;&lt;｜tool▁outputs▁end｜&gt;&#39; + message[&#39;content&#39;] + &#39;&lt;｜end▁of▁sentence｜&gt;&#39;}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[&#39;content&#39;] %}{% if &#39;&lt;/think&gt;&#39; in content %}{% set content = content.split(&#39;&lt;/think&gt;&#39;)[-1] %}{% endif %}{{&#39;&lt;｜Assistant｜&gt;&#39; + content + &#39;&lt;｜end▁of▁sentence｜&gt;&#39;}}{%- endif %}{%- endif %}{%- if message[&#39;role&#39;] == &#39;tool&#39; %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{&#39;&lt;｜tool▁outputs▁begin｜&gt;&lt;｜tool▁output▁begin｜&gt;&#39; + message[&#39;content&#39;] + &#39;&lt;｜tool▁output▁end｜&gt;&#39;}}{%- set ns.is_output_first = false %}{%- else %}{{&#39;&lt;｜tool▁output▁begin｜&gt;&#39; + message[&#39;content&#39;] + &#39;&lt;｜tool▁output▁end｜&gt;&#39;}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{&#39;&lt;｜tool▁outputs▁end｜&gt;&#39;}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{&#39;&lt;｜Assistant｜&gt;&#39;}}{% endif %}&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><hr><ul class=pager><li class=previous><a href=/post/2025-02-23-a800-deepseek-awq/ data-toggle=tooltip data-placement=top title="A800 单机8卡体验 DeepSeek-R1-AWQ 量化满血版之旅">&larr;
Previous Post</a></li><li class=next><a href=/post/2025-03-08-llm-learn2/ data-toggle=tooltip data-placement=top title="LLM 教程（2）- 大模型基础知识">Next
Post &rarr;</a></li></ul></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"></div></div></div></article><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:tanjunchen20@gmail.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/tanjunchen><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; 陈谭军 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,r=$(_containerSelector),a=r.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),a.each(function(){n=$(this).prop("tagName").toLowerCase(),i="#"+$(this).prop("id"),s=$(this).text(),t=$('<a href="'+i+'" rel="nofollow">'+s+"</a>"),o=$('<li class="'+n+'_nav"></li>').append(t),$(e).append(o)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script></body></html>