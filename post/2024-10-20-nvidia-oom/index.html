<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="漫步远方，心荡神往"><meta property="og:type" content="article"><meta property="og:image" content="https://tanjunchen.github.io/img/home.webp"><meta property="twitter:image" content="https://tanjunchen.github.io/img/home.webp"><meta name=title content="记一次 NVIDIA 卡训练任务出现 OOM 排查过程和解决思路"><meta property="og:title" content="记一次 NVIDIA 卡训练任务出现 OOM 排查过程和解决思路"><meta property="twitter:title" content="记一次 NVIDIA 卡训练任务出现 OOM 排查过程和解决思路"><meta name=description content="本次OOM问题表现为GPU显存充足但PyTorch无法申请，具有概率性、偶发性特征。排查覆盖资源限制、容器运行时、GPU插件、CUDA环境、驱动、内核、存储网络等多个层面，排除了主存限制、存储I/O、网络及runtime实现问题。关键线索包括：nvidia-smi显示初始化显存异常（仅4M）、旧内核（3.10）与新驱动（535+）兼容性存疑、GPU共享模式干扰、前后脚本连续执行可能导致CUDA上下文污染。综合判断，问题核心并非真实显存不足，而是CUDA上下文初始化失败或状态异常，由前置脚本隐式加载CUDA、GPU虚拟化干扰、旧内核兼容性不足等多重因素叠加导致，属于“伪OOM”。"><meta property="og:description" content="本次OOM问题表现为GPU显存充足但PyTorch无法申请，具有概率性、偶发性特征。排查覆盖资源限制、容器运行时、GPU插件、CUDA环境、驱动、内核、存储网络等多个层面，排除了主存限制、存储I/O、网络及runtime实现问题。关键线索包括：nvidia-smi显示初始化显存异常（仅4M）、旧内核（3.10）与新驱动（535+）兼容性存疑、GPU共享模式干扰、前后脚本连续执行可能导致CUDA上下文污染。综合判断，问题核心并非真实显存不足，而是CUDA上下文初始化失败或状态异常，由前置脚本隐式加载CUDA、GPU虚拟化干扰、旧内核兼容性不足等多重因素叠加导致，属于“伪OOM”。"><meta property="twitter:description" content="本次OOM问题表现为GPU显存充足但PyTorch无法申请，具有概率性、偶发性特征。排查覆盖资源限制、容器运行时、GPU插件、CUDA环境、驱动、内核、存储网络等多个层面，排除了主存限制、存储I/O、网络及runtime实现问题。关键线索包括：nvidia-smi显示初始化显存异常（仅4M）、旧内核（3.10）与新驱动（535+）兼容性存疑、GPU共享模式干扰、前后脚本连续执行可能导致CUDA上下文污染。综合判断，问题核心并非真实显存不足，而是CUDA上下文初始化失败或状态异常，由前置脚本隐式加载CUDA、GPU虚拟化干扰、旧内核兼容性不足等多重因素叠加导致，属于“伪OOM”。"><meta property="twitter:card" content="summary"><meta name=keyword content="陈谭军, 互联网, 云原生, 容器, 微服务, Web, PaaS, Istio, Kubernetes, Microservice"><link rel="shortcut icon" href=/img/favicon.ico><title>记一次 NVIDIA 卡训练任务出现 OOM 排查过程和解决思路 | 陈谭军的博客 | tanjunchen Blog</title><link rel=canonical href=/post/2024-10-20-nvidia-oom/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script>
<script src=/js/bootstrap.min.js></script>
<script src=/js/hux-blog.min.js></script>
<script src=/js/lazysizes.min.js></script></head><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button>
<a class=navbar-brand href=/>漫步远方，心荡神往</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/technology/>technology</a></li><li><a href=/categories/think/>think</a></li><li><a href=/learning/>LEARNING</a></li><li><a href=/archive/>ARCHIVE</a></li><li><a href=/about/>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(/img/home.webp)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/kubernetes title=kubernetes>kubernetes</a>
<a class=tag href=/tags/nvidia title=NVIDIA>NVIDIA</a>
<a class=tag href=/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B title=大模型>大模型</a></div><h1>记一次 NVIDIA 卡训练任务出现 OOM 排查过程和解决思路</h1><h2 class=subheading>要求机器配置、驱动、CUDA、PyTorch、大模型、OOM、GPU、容器、虚拟化、内核、存储网络、兼容性、初始化、共享模式等配置相辅相成，排查过程复杂，需综合分析</h2><span class=meta>Posted by
陈谭军
on
Sunday, October 20, 2024
<span id=busuanzi_container_page_pv>|<span id=busuanzi_value_page_pv></span><span>
<span id=/post/2024-10-20-nvidia-oom/ class="leancloud_visitors meta_data_item" data-flag-title><span class=post-meta-item-icon><span class="octicon octicon-eye"></span></span>
<i class="fa fa-eye"></i>
<span class=old-visitors-count style=display:none></span>
<span class=leancloud-visitors-count></span></span>
<script src=https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js></script>
<script>AV.initialize("","")</script><script type=text/javascript>function showTime(e){var n=new AV.Query(e),t=[],s=$(".leancloud_visitors");s.each(function(){t.push($(this).attr("id").trim())}),n.containedIn("url",t),n.find().done(function(e){for(var s,o,i,a,r,c,l=".leancloud-visitors-count",d=".old-visitors-count",n=0;n<e.length;n++)a=e[n],i=a.get("url"),c=a.get("time"),s=document.getElementById(i),$(s).find(l).text(c);for(n=0;n<t.length;n++)i=t[n],s=document.getElementById(i),o=$(s).find(l),o.text()==""&&(r=$(s).find(d).text(),r!=""?o.text(0+parseInt(r)):o.text(0))}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var n=$(".leancloud_visitors"),t=n.attr("id").trim(),o=n.attr("data-flag-title").trim(),s=new AV.Query(e);s.equalTo("url",t),s.find({success:function(n){if(n.length>0){var s,i,r,c,l,a=n[0];a.fetchWhenSave(!0),a.increment("time"),a.save(null,{success:function(e){var n=$(document.getElementById(t));n.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else s=new e,i=new AV.ACL,i.setPublicReadAccess(!0),i.setPublicWriteAccess(!0),s.setACL(i),s.set("title",o),s.set("url",t),c=".old-visitors-count",l=$(document.getElementById(t)),r=l.find(c).text(),r!=""?s.set("time",parseInt(r)+1):s.set("time",1),s.save(null,{success:function(e){var n=$(document.getElementById(t));n.find(".leancloud-visitors-count").text(e.get("time"))},error:function(){console.log("Failed to create")}})},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");$(".leancloud_visitors").length==1?addCount(e):showTime(e)})</script>阅读 </span></span>|<span class=post-date>共 3520 字</span>，阅读约 <span class=more-meta>8 分钟</span></span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><h1 id=1-问题>1. 问题</h1><p>有客户反馈使用 A100 卡跑训练时会出现 RuntimeError: CUDA error: out of memory 报错。使用 docker Pod 运行的方式下出现的概率很低，但是使用 Kubernetes Pod 会很大的概率出现上述报错现象，总之都会出现该报错。</p><p>具体报错信息如下所示：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Traceback <span style=color:#ff79c6>(</span>most recent call last<span style=color:#ff79c6>)</span>:
</span></span><span style=display:flex><span>  File <span style=color:#f1fa8c>&#34;tools/test.py&#34;</span>, line 195, in &lt;module&gt;
</span></span><span style=display:flex><span>    main<span style=color:#ff79c6>()</span>
</span></span><span style=display:flex><span>  File <span style=color:#f1fa8c>&#34;tools/test.py&#34;</span>, line 182, in main
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>runner</span> <span style=color:#ff79c6>=</span> Runner.from_cfg<span style=color:#ff79c6>(</span>cfg<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>  File <span style=color:#f1fa8c>&#34;/tmp/algorithm/deps/mmengine/runner/runner.py&#34;</span>, line 443, in from_cfg
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>runner</span> <span style=color:#ff79c6>=</span> cls<span style=color:#ff79c6>(</span>
</span></span><span style=display:flex><span>  File <span style=color:#f1fa8c>&#34;/tmp/algorithm/deps/mmengine/runner/runner.py&#34;</span>, line 412, in __init__
</span></span><span style=display:flex><span>    self.model <span style=color:#ff79c6>=</span> self.wrap_model<span style=color:#ff79c6>(</span>
</span></span><span style=display:flex><span>  File <span style=color:#f1fa8c>&#34;/tmp/algorithm/deps/mmengine/runner/runner.py&#34;</span>, line 855, in wrap_model
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>model</span> <span style=color:#ff79c6>=</span> model.to<span style=color:#ff79c6>(</span>get_device<span style=color:#ff79c6>())</span>
</span></span><span style=display:flex><span>  File <span style=color:#f1fa8c>&#34;/tmp/algorithm/deps/mmengine/model/base_model/base_model.py&#34;</span>, line 202, in to
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> super<span style=color:#ff79c6>()</span>.to<span style=color:#ff79c6>(</span>*args, **kwargs<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>  File <span style=color:#f1fa8c>&#34;/opt/conda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py&#34;</span>, line 899, in to
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> self._apply<span style=color:#ff79c6>(</span>convert<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>  File <span style=color:#f1fa8c>&#34;/opt/conda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py&#34;</span>, line 570, in _apply
</span></span><span style=display:flex><span>    module._apply<span style=color:#ff79c6>(</span>fn<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>  File <span style=color:#f1fa8c>&#34;/opt/conda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py&#34;</span>, line 570, in _apply
</span></span><span style=display:flex><span>    module._apply<span style=color:#ff79c6>(</span>fn<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>  File <span style=color:#f1fa8c>&#34;/opt/conda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py&#34;</span>, line 570, in _apply
</span></span><span style=display:flex><span>    module._apply<span style=color:#ff79c6>(</span>fn<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>[</span>Previous line repeated <span style=color:#bd93f9>1</span> more time<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span>  File <span style=color:#f1fa8c>&#34;/opt/conda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py&#34;</span>, line 593, in _apply
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>param_applied</span> <span style=color:#ff79c6>=</span> fn<span style=color:#ff79c6>(</span>param<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>  File <span style=color:#f1fa8c>&#34;/opt/conda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py&#34;</span>, line 897, in convert
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> t.to<span style=color:#ff79c6>(</span>device, dtype <span style=color:#ff79c6>if</span> t.is_floating_point<span style=color:#ff79c6>()</span> or t.is_complex<span style=color:#ff79c6>()</span> <span style=color:#ff79c6>else</span> None, non_blocking<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>RuntimeError: CUDA error: out of memory
</span></span></code></pre></div><h1 id=2-分析过程>2. 分析过程</h1><p>客户训练镜像与机器基础信息：</p><ul><li>内核：3.10.0-1062.9.1.el7.x86_64</li><li>宿主机：CentOS Linux release 7.3.1611 (Core) 与 cuda_11.8.r11.8/compiler.31833905_0</li><li>容器镜像：Ubuntu 20.04.6 LTS \n \l 与 cuda_11.3.r11.3/compiler.29920130_0</li><li>k8s：1.20.8</li><li>device-plugin：nvidia-k8s-device-plugin:1.11</li><li>Driver Version: 535.129.03</li><li>CUDA Version: 12.2</li><li>PyTorch: 1.10.1+cu113</li><li>CuDNN：8.2</li><li>Python: 3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]</li><li>NVCC: Cuda compilation tools, release 11.3, V11.3.109</li></ul><p>搜索引擎与 NVIDIA 官网给出的建议是升级 torch 版本（pytorch:2.4.1），升级版本后会打印出更详细的报错日志，但是因不可控因素任务镜像无法升级 torch 版本，所以需要协助客户分析出现这个报错的原因与解决该问题；</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>OutOfMemoryError: CUDA out of memory. 
</span></span><span style=display:flex><span>Tried to allocate 734.00 MiB <span style=color:#ff79c6>(</span>GPU 0; 7.79 GiB total capacity; 
</span></span><span style=display:flex><span>5.20 GiB already allocated; 139.94 MiB free;6.78 GiB reserved in total by PyTorch<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  
</span></span><span style=display:flex><span>See documentation <span style=color:#ff79c6>for</span> Memory Management and PYTORCH_CUDA_ALLOC_CONF；
</span></span></code></pre></div><h2 id=21-排查思路>2.1. 排查思路</h2><p>按照以下思路进行排查：</p><table><thead><tr><th>类别</th><th>排查项</th><th>排查方式</th><th>排查结果</th></tr></thead><tbody><tr><td><strong>K8s & 容器资源限制</strong></td><td>CPU/Memory 限制</td><td>检查 Pod 和 Docker 的 resource limits</td><td>存在内存限制，但 OOM 发生在 GPU 显存申请阶段，且主内存使用未达限值，排除主内存不足导致 OOM 的可能</td></tr><tr><td><strong>Docker & 容器运行时</strong></td><td>nvidia-container-runtime</td><td>替换为原生版本、对比容器 <code>inspect</code> 信息、OCI 配置、nvidia-container-runtime 执行日志</td><td>自研 runtime 与原生版本在配置和行为上无明显差异，基本排除容器运行时实现问题</td></tr><tr><td><strong>GPU 插件与虚拟化</strong></td><td>gpu-manager、nvidia-device-plugin、共享模式</td><td>关闭 gpu-manager 组件、尝试直接挂载 GPU 设备（绕过 device plugin）</td><td>关闭共享模式后问题曾短暂消失，后续复现，表明 GPU 虚拟化层可能存在资源隔离或上下文管理异常，存在干扰嫌疑</td></tr><tr><td><strong>CUDA 环境</strong></td><td>宿主机 vs 容器内 CUDA 版本</td><td>宿主机 CUDA 12.2，容器内 CUDA 11.3</td><td>版本不一致，虽符合 NVIDIA 官方兼容性矩阵（驱动 ≥ 所需最低版本），但在复杂调度与多容器场景下存在潜在兼容风险，尤其影响 CUDA context 初始化行为</td></tr><tr><td><strong>NVIDIA 驱动</strong></td><td>驱动版本（535.183.01 / 520 / 525）</td><td>升级/降级驱动版本进行对比测试</td><td>高版本驱动（535.183.01）下 <code>nvidia-smi</code> 显示 GPU 初始化显存仅 4M（异常，正常应为数十 MB 起），表明设备初始化异常；降级至 520 导致业务失败，525 仍存在问题，驱动与旧内核协同存在隐患</td></tr><tr><td><strong>存储 & 网络</strong></td><td>PFS 存储、CNI 网络</td><td>将 PFS 数据迁移至本地磁盘；将 Pod 网络模式由 CNI 切换为 hostNetwork</td><td>问题依旧存在，排除 PFS 网络延迟或文件锁竞争导致阻塞的可能性</td></tr><tr><td><strong>PyTorch 框架行为</strong></td><td>torch 1.10.1、CUDA 初始化逻辑</td><td>在代码中增加 <code>try-catch</code>、手动调用 <code>torch.cuda.init()</code>、循环查询显存状态</td><td><code>nvidia-ml-py</code>（通过 <code>pynvml</code>）可正常读取显存信息，但 <code>torch.cuda</code> 申请显存失败，表明问题发生在 <strong>PyTorch CUDA 上下文初始化层</strong>，而非底层驱动或硬件</td></tr><tr><td><strong>业务代码结构</strong></td><td>业务代码是否存在缺陷（但在其他厂商环境运行正常）</td><td>对比不同厂商环境的基础软硬件配置差异</td><td>代码本身在其他环境运行稳定，初步排除逻辑 bug；问题更可能源于当前环境基础配置（如内核版本、驱动、GPU 虚拟化策略）未与其他厂商对齐，导致行为偏移</td></tr></tbody></table><p><em><strong>经过上述排查过程后，发现问题依然存在，但还是没有定位到原因与解决办法。</strong></em></p><h2 id=22-核心突破>2.2. 核心突破</h2><p>客户无意中提到同样的代码在另外一套环境基本不会出现 OOM，通过对比差异点，发现机器的内核版本不同，没有问题的内核版本是 4.18，现在有问题的机器内核是 3.10。</p><p>【突破点，可能与内核版本有关】查阅官网，内核与cuda版本兼容文档如下所示：</p><p><img src=/images/2024-10-20-nvidia-oom/1.png alt></p><p><img src=/images/2024-10-20-nvidia-oom/2.png alt></p><p><em><strong>现在机器的内核版本不满足表格中的版本要求，我们尝试升级内核版本到 4.19，经过大量测试与验证发现问题得到解决；</strong></em></p><p>【原因分析】
在某种场景下（业务 torch 版本 ：1.10 cuda 版本： cuda_11.3.r11.3/compiler.29920130_0 操作系统：ubuntu20.04）在宿主机驱动版本： CUDA Driver 535.183.x ，cuda 版本：cuda_11.8.r11.8/compiler.31833905_0）由于内核版本偏低可能触发了bug，导致在k8s或者docker容器运行方式下出现该OOM问题。</p><h1 id=3-结论与思考>3. 结论与思考</h1><table><thead><tr><th>怀疑原因</th><th>支持证据</th><th>置信度</th><th>备注</th></tr></thead><tbody><tr><td><strong>旧内核（3.10）与新驱动不兼容导致 GPU 初始化异常（核心原因）</strong></td><td>- 系统偶现内核态 hang 住现象<br>- <code>nvidia-smi</code> 显示 GPU 初始化后显存仅 4M（严重异常，正常应为数十 MB 起）<br>- 升级内核过程中问题频率下降，初步验证相关性</td><td>🔴 高</td><td>内核 3.10 发布于 2013 年，而当前使用驱动版本为 535.183.01（2023+），存在显著版本错配，可能导致 GPU 内存映射、DMA 或设备初始化异常</td></tr><tr><td><strong>PyTorch 1.10.1 在低内核 + 虚拟化环境下的 CUDA 初始化缺陷</strong></td><td>- 在 <code>tools/test.py</code> 中手动调用 <code>torch.cuda.init()</code> 有时成功、有时失败<br>- 增加 <code>try-catch</code> 后可捕获 OOM，但底层 <code>pynvml</code> 显示显存充足<br>- 问题在非虚拟化环境或高内核版本下较少出现</td><td>🟡 中高</td><td>PyTorch 1.10.1 对复杂环境（老旧内核 + GPU 虚拟化）的容错能力较弱，CUDA context 初始化可能因底层状态不一致而失败</td></tr><tr><td><strong>前置脚本污染 CUDA 上下文 + GPU 共享模式干扰</strong></td><td>- <code>bin2pcd.py</code> 执行后立即运行 <code>tools/test.py</code> 时更容易触发 OOM<br>- 关闭 <code>gpu-manager</code>（即禁用 GPU 共享模式）后问题曾短暂消失<br>- 拆分两个脚本执行环境后问题减少</td><td>🟡 中</td><td>即使 <code>bin2pcd.py</code> 未显式使用 GPU，若其导入了 <code>torch</code> 等库，可能触发隐式 CUDA context 初始化且未正确释放，导致后续进程无法正常申请资源；共享模式加剧了上下文管理复杂性</td></tr></tbody></table><p>本次排查遵循“由外到内、逐层剥离、控制变量、交叉验证”的系统性方法，全面覆盖了从应用层到内核底层的全技术栈，体现了典型的复杂环境故障定位思路。</p><ul><li>分层排查，边界清晰：将整个技术栈划分为多个独立层级（K8s 资源、容器运行时、GPU插件、CUDA环境、驱动、存储、网络等），逐层验证，确保每一层的问题被独立评估，避免干扰判断。这种“模块化隔离”策略有效防止了误判和路径偏移。</li><li>控制变量，精准对比：在关键环节（如容器运行时替换、数据路径迁移、GPU挂载方式切换）中，通过环境对调、配置替换、组件回退等方式实现变量控制。例如：<ul><li>替换自研与原生 nvidia-container-runtime 对比行为差异；</li><li>将PFS数据迁移至本地盘以排除网络存储影响；</li><li>交换机器验证是否为硬件个体问题；这些操作体现了严谨的实验设计思维。</li></ul></li><li>现象驱动，逆向溯源：面对“偶发性OOM”这一非确定性问题，通过增加日志埋点、try-catch捕获、显存轮询、自助初始化CUDA等手段，主动触发并捕获异常现场，将“黑盒问题”逐步转化为可观测行为，极大提升了问题可分析性。</li><li>兼容性与版本敏感性验证：针对异构环境（宿主机CUDA 12.2 / 容器内11.3、驱动535+ / 内核3.10），评估版本兼容性风险，并通过升级/降级驱动、尝试不同内核版本等方式验证假设，体现出对软硬件协同依赖的深刻理解。</li><li>对虚拟化中间层保持警惕：gpu-manager 和共享模式可能引入的副作用，通过关闭组件、直连设备等方式绕过抽象层，验证了“中间件干扰”的可能性。这是在云原生AI场景中尤为关键的排查视角。</li><li>具备底层系统视野：当问题指向内核hang、nvidia-smi初始化异常等低层现象时，跳出框架层，深入到内核版本、DMA、设备初始化流程等操作系统层面进行推断，展现了对GPU资源管理全链路的理解。</li><li>接受不确定性，保留回溯路径：面对“可复现→消失”的波动现象，未强行归因，而是通过多轮交叉验证积累证据链，保留多种可能性，体现了工程排查中的理性与克制。</li></ul><p>现象：GPU显存充足（nvidia-smi 显示有空闲），但 torch.cuda 申请显存失败（OOM）；问题偶发、替换机器、迁移数据、更换 runtime、升级/降级驱动、切换网络模式等操作均不能彻底解决</p><p>核心难点：GPU显存充足但PyTorch无法申请显存（OOM），且具有概率性、偶发性、不可稳定复现的特点；系统层面显存可用，但 PyTorch CUDA 上下文申请失败，说明问题可能发生在 CUDA 上下文管理、内存管理机制、或资源未释放导致的隐式占用；</p><p><strong>总结：本次OOM问题表现为GPU显存充足但PyTorch无法申请，具有概率性、偶发性特征。排查覆盖资源限制、容器运行时、GPU插件、CUDA环境、驱动、内核、存储网络等多个层面，排除了主存限制、存储I/O、网络及runtime实现问题。关键线索包括：nvidia-smi显示初始化显存异常（仅4M）、旧内核（3.10）与新驱动（535+）兼容性存疑、GPU共享模式干扰、前后脚本连续执行可能导致CUDA上下文污染。综合判断，问题核心并非真实显存不足，而是CUDA上下文初始化失败或状态异常，由前置脚本隐式加载CUDA、GPU虚拟化干扰、旧内核兼容性不足等多重因素叠加导致，属于“伪OOM”。</strong></p><hr><ul class=pager><li class=previous><a href=/post/2024-05-02-linux-page-fault/ data-toggle=tooltip data-placement=top title="监测 Linux 内存缺页中断事件">&larr;
Previous Post</a></li><li class=next><a href=/post/2024-10-31-gpu-diagnosis/ data-toggle=tooltip data-placement=top title="NVIDIA GPU 系统诊断与运维排查手册：常用命令与一键脚本指南">Next
Post &rarr;</a></li></ul></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"></div></div></div></article><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:tanjunchen20@gmail.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/tanjunchen><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; 陈谭军 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,r=$(_containerSelector),a=r.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),a.each(function(){n=$(this).prop("tagName").toLowerCase(),i="#"+$(this).prop("id"),s=$(this).text(),t=$('<a href="'+i+'" rel="nofollow">'+s+"</a>"),o=$('<li class="'+n+'_nav"></li>').append(t),$(e).append(o)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script></body></html>