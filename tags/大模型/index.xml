<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>大模型 on 漫步远方，心荡神往</title><link>https://tanjunchen.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/</link><description>Recent content in 大模型 on 漫步远方，心荡神往</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>陈谭军</copyright><lastBuildDate>Sun, 01 Jun 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://tanjunchen.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/index.xml" rel="self" type="application/rss+xml"/><item><title>探索 Transformer 理论与本质</title><link>https://tanjunchen.github.io/post/2025-06-01-transformer-01/</link><pubDate>Sun, 01 Jun 2025 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2025-06-01-transformer-01/</guid><description>1. 语言模型训练和推理 一般来说，语言模型旨在对于人类语言的内在规律进行建模，从而准确预测词序列中未来（或缺失）词或词元（Token）的概率。根</description></item><item><title>科普开源大模型基础知识</title><link>https://tanjunchen.github.io/post/2025-05-07-llm-01/</link><pubDate>Wed, 07 May 2025 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2025-05-07-llm-01/</guid><description>Llama 4 北京时间2025年4月6日凌晨，Meta发布了外界期待许久的Llama4系列开源模型，目前它包括 Llama 4 Scout、Llama 4 Maveri</description></item><item><title>DeepSeek 开源周活动</title><link>https://tanjunchen.github.io/post/2025-05-01-deepseek-weekly/</link><pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2025-05-01-deepseek-weekly/</guid><description>1. DeepSeek 开源周 DeepSeek 在开源了 DeepSeek-R1 与 DeepSeek-V3 模型权重后，DeepSeek-V3 技术报告 《DeepSeek-V3 Technical Report》 中提到的很多核心技术，相继在 &amp;ldquo;DeepSeek 开</description></item><item><title>双机2*H20(8*96GiB)部署满血DeepSeek-R1(fp8)验证过程</title><link>https://tanjunchen.github.io/post/2025-04-05-2h20-deepseek-r1-sglang-vllm/</link><pubDate>Sat, 05 Apr 2025 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2025-04-05-2h20-deepseek-r1-sglang-vllm/</guid><description>环境信息 机器配置 OS：CentOS Linux release 7.6 (Final) Kernel：4.19.0-1.0.0.9 驱动： Driver Version: 535.216.03 CUDA Version: 12.2 GPU：NVIDIA H20 vLLM：htt</description></item><item><title>单机H20(8*96GiB)部署满血DeepSeek-R1(fp8)验证过程</title><link>https://tanjunchen.github.io/post/2025-04-04-h20-deepseek-r1-sglang-vllm/</link><pubDate>Fri, 04 Apr 2025 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2025-04-04-h20-deepseek-r1-sglang-vllm/</guid><description>环境信息 机器配置 OS：CentOS Linux release 7.6 (Final) Kernel：4.19.0-1.0.0.9 驱动： Driver Version: 535.216.03 CUDA Version: 12.2 GPU：NVIDIA H20 vLLM：htt</description></item><item><title>单台 H20 机器 DeepSeek R1 (FP8)、DeepSeek-R1-Block-INT8 精度测试与性能测试过程</title><link>https://tanjunchen.github.io/post/2025-03-23-h20-deepseek-r1/</link><pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2025-03-23-h20-deepseek-r1/</guid><description>测试目标 测试下 DeepSeek R1（FP8） 使用单台 H20 机器在 aime、math500、gpqa (使用开源工具 evalscope) 数据集下进行精度测试； 给定输入、输出等参数，</description></item><item><title>深度剖析 DeepSeek R1 论文</title><link>https://tanjunchen.github.io/post/2025-03-16-llm-deepseek-r1/</link><pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2025-03-16-llm-deepseek-r1/</guid><description>1. DeepSeek R1 论文精读 1.1. 摘要 DeepSeek 推出了他们的第一代推理模型，DeepSeek-R1-Zero和DeepSeek-R1； DeepSeek-R1-Zero 这是一个跳过监督微调（SFT）</description></item><item><title>DeepSeek R1 推理 GPU 资源配置</title><link>https://tanjunchen.github.io/post/2025-03-15-deepseek-r1-gpu-resource/</link><pubDate>Sat, 15 Mar 2025 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2025-03-15-deepseek-r1-gpu-resource/</guid><description>Deepseek R1 推理资源需求 量化与 FP8 动态转换： 暂不考虑量化技术； FP8 动态转换因性能损失较大，不推荐使用。因此，默认情况下，不具备 FP8 计算单元的显卡无法运行 FP8</description></item><item><title>LLM 教程（3）- 《DeepSeek R1 论文精读 - 通过强化学习推动大语言模型推理能力的突破与创新》</title><link>https://tanjunchen.github.io/post/2025-03-09-llm-deepseek-r1/</link><pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2025-03-09-llm-deepseek-r1/</guid><description>1. 序言 下图展示了 OpenAI（公开文献） 从预训练开始逐步训练出一个 GPT 助手的步骤；pre-training -&amp;gt; SFT -&amp;gt; RM -&amp;gt; RL 是典型的大模型训练过程。</description></item><item><title>LLM 教程（2）- 大模型基础知识</title><link>https://tanjunchen.github.io/post/2025-03-08-llm-learn2/</link><pubDate>Sat, 08 Mar 2025 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2025-03-08-llm-learn2/</guid><description>LLM 专有名词 量化（Quantization） 基础知识 LLM 大模型的量化技术主要是通过对模型参数进行压缩和量化，从而降低模型的存储和计算复杂度。具体</description></item><item><title>LLM 教程（1）- DeepSeek-R1 初步入门</title><link>https://tanjunchen.github.io/post/2025-03-01-llm-learn1/</link><pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2025-03-01-llm-learn1/</guid><description>基础知识 查看 deepseek-ai 开源官网，DeepSeek 有以下系列： DeepSeek-R1 DeepSeek-V3 （DeepSeek-V3-Base） DeepSeek-VL DeepSeek-Coder DeepSeek-Math DeepSeek-LLM 蒸馏模型系列（Qwen、LLaMA等） &amp;hellip;&amp;hellip;</description></item><item><title>A800 单机8卡体验 DeepSeek-R1-AWQ 量化满血版之旅</title><link>https://tanjunchen.github.io/post/2025-02-23-a800-deepseek-awq/</link><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2025-02-23-a800-deepseek-awq/</guid><description>硬件与系统环境要求 硬件配置 GPU: 8× NVIDIA A800 80GB 显存要求: 每卡80GB 系统内存: ≥32GB (用于交换空间) CPU：lscpu | grep &amp;ldquo;Model name&amp;rdquo; 值：Model name: Intel(R)</description></item><item><title>vLLM 多机多卡推理测试与验证（Docker）</title><link>https://tanjunchen.github.io/post/2025-01-19-inference-serve/</link><pubDate>Sun, 19 Jan 2025 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2025-01-19-inference-serve/</guid><description>基础概念 【分布式推理与服务】（Distributed Inference and Serving）是指在多个机器或设备之间部署和管理机器学习模型，以高效地处理推理请求</description></item><item><title>vLLM 多机多卡推理测试与验证（Kubernetes）</title><link>https://tanjunchen.github.io/post/2025-01-19-inference-serve-k8s/</link><pubDate>Sun, 19 Jan 2025 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2025-01-19-inference-serve-k8s/</guid><description>基础概念 【分布式推理与服务】（Distributed Inference and Serving）是指在多个机器或设备之间部署和管理机器学习模型，以高效地处理推理请求</description></item><item><title>云原生 AI 能力引擎（大模型 AI 基础套件）</title><link>https://tanjunchen.github.io/post/2025-01-05-ai-infra/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2025-01-05-ai-infra/</guid><description>本文详尽列举了构建和实施先进人工智能（AI）解决方案所需的关键技术组件。 首先，针对单机环境，文档罗列了并行计算平台、GPU驱动程序、容器化工</description></item><item><title>NVIDIA GPU 系统诊断与运维排查手册：常用命令与一键脚本指南</title><link>https://tanjunchen.github.io/post/2024-10-31-gpu-diagnosis/</link><pubDate>Sat, 26 Oct 2024 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2024-10-31-gpu-diagnosis/</guid><description>GPU 常见排查工具与命令 查看系统信息 检查项 命令/方法 说明 硬件厂商 dmidecode -t system (提取 Manufacturer) 获取系统制造商信息 CPU 型号 awk -F': ' '/model name/{print $2; exit}' /proc/cpuinfo 提取 CPU 型号名称 操作系统 awk -F'&amp;quot;'</description></item><item><title>记一次 NVIDIA 卡训练任务出现 OOM 排查过程和解决思路</title><link>https://tanjunchen.github.io/post/2024-10-20-nvidia-oom/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://tanjunchen.github.io/post/2024-10-20-nvidia-oom/</guid><description>1. 问题 有客户反馈使用 A100 卡跑训练时会出现 RuntimeError: CUDA error: out of memory 报错。使用 docker Pod 运行的方式下出现的概率很低，但是使用 Kubernetes Pod 会很大的概率出现上述报错现象，总之都</description></item></channel></rss>