<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="漫步远方，心荡神往"><meta property="og:type" content="article"><meta property="og:image" content="https://tanjunchen.github.io/img/home.webp"><meta property="twitter:image" content="https://tanjunchen.github.io/img/home.webp"><meta name=title content="AI"><meta property="og:title" content="AI"><meta property="twitter:title" content="AI"><meta name=description content="陈谭军，软件工程师, 开源爱好者, 互联网, 云原生, 容器, 微服务, Web, PaaS, Istio, Kubernetes, Microservice"><meta property="og:description" content="陈谭军，软件工程师, 开源爱好者, 互联网, 云原生, 容器, 微服务, Web, PaaS, Istio, Kubernetes, Microservice"><meta property="twitter:description" content="陈谭军，软件工程师, 开源爱好者, 互联网, 云原生, 容器, 微服务, Web, PaaS, Istio, Kubernetes, Microservice"><meta property="twitter:card" content="summary"><meta name=keyword content="陈谭军, 互联网, 云原生, 容器, 微服务, Web, PaaS, Istio, Kubernetes, Microservice"><link rel="shortcut icon" href=/img/favicon.ico><title>AI | 陈谭军的博客 | tanjunchen Blog</title><link rel=canonical href=/tags/ai/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script>
<script src=/js/bootstrap.min.js></script>
<script src=/js/hux-blog.min.js></script>
<script src=/js/lazysizes.min.js></script></head><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button>
<a class=navbar-brand href=/>漫步远方，心荡神往</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/technology/>technology</a></li><li><a href=/categories/think/>think</a></li><li><a href=/learning/>LEARNING</a></li><li><a href=/archive/>ARCHIVE</a></li><li><a href=/about/>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><header class=intro-header style=background-image:url(/img/home.webp)><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=site-heading><h1>漫步远方，心荡神往</h1><span class=subheading></span></div></div></div></div></header><div data-pagefind-ignore=all class=container><div class=row><div class="col-lg-8 col-lg-offset-1
col-md-8 col-md-offset-1
col-sm-12
col-xs-12
post-container"><div data-pagefind-ignore=all><div class=post-preview><a href=https://tanjunchen.github.io/post/2025-06-01-transformer-01/><h2 class=post-title>探索 Transformer 理论与本质</h2><h3 class=post-subtitle>介绍 Transformer 架构和原理，以及大语言模型（LLM）的运作机制。</h3><div class=post-content-preview>大语言模型（LLM）的核心是通过自回归方式逐词预测（next token prediction）。文本首先被 tokenizer 拆分为词或子词（如 BPE、BBPE 技术），每个 token 对应一个嵌入向量，并加入位置编码（如 RoPE）以保留顺序信息。模型基于 Transformer 结构，训练时通过注意力机制学习上下文关系，输出每个 token 的下一个词概率分布（softmax 归一化）。推理时采用自回归生成，通过采样策略（如 Top-k）和温度系数控制随机性。ALiBi 技术解决了长文本位置编码的外推问题，使模型能处理超越训练长度的输入。整个过程本质是序列条件概率建模，通过海量数据学习语言的统计规律。</div></a><p class=post-meta>Posted by 陈谭军 on Sunday, June 1, 2025</p></div><hr><div class=post-preview><a href=https://tanjunchen.github.io/post/2025-05-07-llm-01/><h2 class=post-title>科普开源大模型基础知识</h2><h3 class=post-subtitle>本篇文章主要介绍开源大模型的基础知识，如 LLama 4 和 Qwen 3 的核心亮点和基础架构。</h3><div class=post-content-preview>Llama 4 北京时间2025年4月6日凌晨，Meta发布了外界期待许久的Llama4系列开源模型，目前它包括 Llama 4 Scout、Llama 4 Maveri</div></a><p class=post-meta>Posted by 陈谭军 on Wednesday, May 7, 2025</p></div><hr><div class=post-preview><a href=https://tanjunchen.github.io/post/2025-05-01-deepseek-weekly/><h2 class=post-title>DeepSeek 开源周活动</h2><h3 class=post-subtitle>在2025年2月24日至28日的DeepSeek开源周期间，DeepSeek集中发布了五大核心开源项目，全面覆盖AI基础设施中的计算优化、通信效率与存储加速等关键领域，构建起一套面向大规模人工智能的高性能技术底座。</h3><div class=post-content-preview>1. DeepSeek 开源周 DeepSeek 在开源了 DeepSeek-R1 与 DeepSeek-V3 模型权重后，DeepSeek-V3 技术报告 《DeepSeek-V3 Technical Report》 中提到的很多核心技术，相继在 &ldquo;DeepSeek 开</div></a><p class=post-meta>Posted by 陈谭军 on Thursday, May 1, 2025</p></div><hr><div class=post-preview><a href=https://tanjunchen.github.io/post/2025-04-05-2h20-deepseek-r1-sglang-vllm/><h2 class=post-title>双机2*H20(8*96GiB)部署满血DeepSeek-R1(fp8)验证过程</h2><h3 class=post-subtitle>双机2*H20(8*96GiB)部署满血DeepSeek-R1(fp8)验证过程、vllm 与 sglang 双机测试与性能对比</h3><div class=post-content-preview>环境信息 机器配置 OS：CentOS Linux release 7.6 (Final) Kernel：4.19.0-1.0.0.9 驱动： Driver Version: 535.216.03 CUDA Version: 12.2 GPU：NVIDIA H20 vLLM：htt</div></a><p class=post-meta>Posted by 陈谭军 on Saturday, April 5, 2025</p></div><hr><div class=post-preview><a href=https://tanjunchen.github.io/post/2025-04-04-h20-deepseek-r1-sglang-vllm/><h2 class=post-title>单机H20(8*96GiB)部署满血DeepSeek-R1(fp8)验证过程</h2><h3 class=post-subtitle>单机H20(8*96GiB)部署满血DeepSeek-R1(fp8)验证过程、vllm 验证过程、sglang 验证过程</h3><div class=post-content-preview>环境信息 机器配置 OS：CentOS Linux release 7.6 (Final) Kernel：4.19.0-1.0.0.9 驱动： Driver Version: 535.216.03 CUDA Version: 12.2 GPU：NVIDIA H20 vLLM：htt</div></a><p class=post-meta>Posted by 陈谭军 on Friday, April 4, 2025</p></div><hr></div><ul class=pager data-pagefind-ignore=all><li class=next><a href=/tags/ai/page/2/>Older Posts &rarr;</a></li></ul></div><div class="col-lg-3 col-lg-offset-0
col-md-3 col-md-offset-0
col-sm-12
col-xs-12
sidebar-container"><section class="visible-md visible-lg"><div class=short-about><a href=/about><img src=/img/tanjunchen.jpg alt=avatar style=cursor:pointer></a><p>Software Developer</p><ul class=list-inline><li><a href=mailto:tanjunchen20@gmail.com><span class="fa-stack fa-lg"><i class="fa fa-circle fa-stack-2x"></i>
<i class="fa fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/tanjunchen><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li></ul></div></section></div></div></div><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:tanjunchen20@gmail.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/tanjunchen><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; 陈谭军 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,r=$(_containerSelector),a=r.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),a.each(function(){n=$(this).prop("tagName").toLowerCase(),i="#"+$(this).prop("id"),s=$(this).text(),t=$('<a href="'+i+'" rel="nofollow">'+s+"</a>"),o=$('<li class="'+n+'_nav"></li>').append(t),$(e).append(o)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script></body></html>